{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning for image classification (CIFAR100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"36\"\n",
    "NAME1 = \"Timothy Hellberg\"\n",
    "NAME2 = \"Lars Liberg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# For dealing with files\n",
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "# For using regex expressions\n",
    "import re\n",
    "\n",
    "# For splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Packages for defining the architecture of our model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, BatchNormalization, Input, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import load_model \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# For generating data\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# One-hot encoding\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Callbacks for training\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ndarray computations\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix for assessment step\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Dataset\n",
    "from keras.datasets import cifar10, cifar100\n",
    "\n",
    "# Other\n",
    "import pandas as pd\n",
    "from subprocess import check_output\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as _Imgdis\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from time import sleep\n",
    "from scipy import ndimage\n",
    "from scipy.misc import imresize\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Average, Dropout, Maximum\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Concatenate\n",
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) | y_train shape: (50000, 100)\n",
      "x_test shape : (10000, 32, 32, 3) | y_test shape : (10000, 1)\n",
      "input_shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "home_path = \"/home/student/deep-machine-learning/Project\"\n",
    "os.chdir(home_path)\n",
    "\n",
    "# Dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=100)\n",
    "print('x_train shape: {} | y_train shape: {}\\nx_test shape : {} | y_test shape : {}'.format(x_train.shape, y_train.shape,                                                                \n",
    "                                                                                            x_test.shape, y_test.shape))\n",
    "input_shape = x_train[0,:,:,:].shape\n",
    "print(\"input_shape:\",input_shape)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "#plt.imshow(x_train[np.random.randint(50000)]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = np.random.randint(50000, size=50000)\n",
    "#x_train_bag = x_train[indices,:,:,:]\n",
    "#y_train_bag = y_train[indices,:]\n",
    "#print(indices)\n",
    "#print(x_train[[1,3,1,3],:,:,1])\n",
    "#rint(y_train[[1,3,1,3],:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Functions, learners and combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal functions\n",
    "\n",
    "def compile_and_train(model, num_epochs): \n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['acc']) \n",
    "    filepath = home_path + '/weights/CIFAR100/' + model.name + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_weights_only=True, save_best_only=True, mode='auto', period=1)\n",
    "    tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=32)\n",
    "    history = model.fit(x=x_train, y=y_train, batch_size=256, epochs=num_epochs, verbose=1, callbacks=[checkpoint, tensor_board], validation_split=0.2)\n",
    "    return history\n",
    "\n",
    "def compile_and_train_bagging(model, num_epochs, x_train, y_train): \n",
    "    indices = np.random.randint(50000, size=50000)\n",
    "    x_train_bag = x_train[indices,:,:,:]\n",
    "    y_train_bag = y_train[indices,:]\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['acc']) \n",
    "    filepath = home_path + '/weights/CIFAR100/' + model.name + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_weights_only=True, save_best_only=True, mode='auto', period=1)\n",
    "    tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=32)\n",
    "    history = model.fit(x=x_train_bag, y=y_train_bag, batch_size=256, epochs=num_epochs, verbose=1, callbacks=[checkpoint, tensor_board], validation_split=0.2)\n",
    "    del x_train_bag, y_train_bag\n",
    "    return history\n",
    "\n",
    "def evaluate_test_accuracy(model):\n",
    "    pred = model.predict(x_test, batch_size = 256)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    pred = np.expand_dims(pred, axis=1) # make same shape as y_test\n",
    "    test_accuracy = np.sum(np.equal(pred, y_test)) / y_test.shape[0]  \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom base learners\n",
    "\n",
    "def base_cnn(model_input):\n",
    "    x = Conv2D(128, (5, 5), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(100, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR100_base_cnn')\n",
    "    return model\n",
    "\n",
    "def base_cnn_2(model_input):\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(100, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR100_base_cnn_2')\n",
    "    return model\n",
    "\n",
    "def base_cnn_2_conv(model_input):\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(100, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR100_base_cnn_2_conv')\n",
    "    return model\n",
    "\n",
    "def base_cnn_3_conv(model_input):\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(100, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR100_base_cnn_3_conv')\n",
    "    return model\n",
    "\n",
    "def conv_pool_cnn(model_input):\n",
    "    x = Conv2D(96, kernel_size=(3, 3), activation='relu', padding =    'same')(model_input)\n",
    "    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (1, 1), activation='relu')(x)\n",
    "    x = Conv2D(100, (1, 1))(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation(activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR100_conv_pool_cnn')\n",
    "    return model\n",
    "\n",
    "def all_cnn(model_input):\n",
    "    x = Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same')(model_input)\n",
    "    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2)(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2)(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (1, 1), activation='relu')(x)\n",
    "    x = Conv2D(100, (1, 1))(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation(activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR100_all_cnn')\n",
    "    return model\n",
    "\n",
    "def base_learner_bagging(model_input, num):\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(100, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR100_base_learner_' + str(num) + '_bagging')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom combiners\n",
    "\n",
    "def average_ensemble(models, model_input):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    #print(np.shape(np.array(outputs)))\n",
    "    #print(outputs[0])\n",
    "    y = Average()(outputs)\n",
    "    model = Model(models[0].input, y, name='CIFAR100_average_ensemble')\n",
    "    return model\n",
    "\n",
    "def concatenate_ensemble(models, model_input):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    print(np.shape(np.array(outputs)))\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    model = Model(model_input, y, name='CIFAR100_concatenate_ensemble')\n",
    "    return model\n",
    "\n",
    "def basic_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(100, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR100_basic_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def small_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(100, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR100_small_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def large_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(100, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR100_large_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def deep_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(100, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR100_deep_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def bn_deep_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(100, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR100_bn_deep_stacking_' + str(num) + '_ensemble')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train base learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 30, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 755,556\n",
      "Trainable params: 754,788\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 14s 344us/step - loss: 3.4154 - acc: 0.2183 - val_loss: 3.2403 - val_acc: 0.2322\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 2.3240 - acc: 0.4366 - val_loss: 2.9324 - val_acc: 0.2938\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 11s 278us/step - loss: 1.5952 - acc: 0.6143 - val_loss: 2.4143 - val_acc: 0.4215\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 11s 281us/step - loss: 1.0108 - acc: 0.7657 - val_loss: 1.9162 - val_acc: 0.5507\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 11s 276us/step - loss: 0.5774 - acc: 0.8750 - val_loss: 2.2469 - val_acc: 0.5204\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 11s 281us/step - loss: 0.2907 - acc: 0.9469 - val_loss: 1.7042 - val_acc: 0.6551\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 11s 276us/step - loss: 0.1213 - acc: 0.9865 - val_loss: 1.5731 - val_acc: 0.6942\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 11s 280us/step - loss: 0.0509 - acc: 0.9968 - val_loss: 1.5286 - val_acc: 0.7050\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 11s 278us/step - loss: 0.0248 - acc: 0.9990 - val_loss: 1.5366 - val_acc: 0.7024\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 0.0155 - acc: 0.9994 - val_loss: 1.5731 - val_acc: 0.7060\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 12s 298us/step - loss: 3.4418 - acc: 0.2078 - val_loss: 3.7133 - val_acc: 0.1751\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 11s 286us/step - loss: 2.3135 - acc: 0.4423 - val_loss: 2.6493 - val_acc: 0.3576\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 1.5780 - acc: 0.6236 - val_loss: 2.8623 - val_acc: 0.3697\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 11s 285us/step - loss: 1.0082 - acc: 0.7639 - val_loss: 2.1795 - val_acc: 0.5209\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 0.5828 - acc: 0.8713 - val_loss: 1.8105 - val_acc: 0.6103\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.2931 - acc: 0.9456 - val_loss: 1.6688 - val_acc: 0.6632\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 12s 290us/step - loss: 0.1321 - acc: 0.9838 - val_loss: 1.6622 - val_acc: 0.6835\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 11s 286us/step - loss: 0.0568 - acc: 0.9960 - val_loss: 1.5470 - val_acc: 0.7011\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 11s 286us/step - loss: 0.0263 - acc: 0.9986 - val_loss: 1.5481 - val_acc: 0.7022\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.0154 - acc: 0.9995 - val_loss: 1.5645 - val_acc: 0.7055\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 12s 305us/step - loss: 3.4456 - acc: 0.2087 - val_loss: 3.1019 - val_acc: 0.2622\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 2.3252 - acc: 0.4344 - val_loss: 2.6770 - val_acc: 0.3497\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 11s 282us/step - loss: 1.6045 - acc: 0.6120 - val_loss: 2.3196 - val_acc: 0.4365\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 11s 282us/step - loss: 1.0455 - acc: 0.7554 - val_loss: 1.8629 - val_acc: 0.5545\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 11s 281us/step - loss: 0.6031 - acc: 0.8664 - val_loss: 1.6849 - val_acc: 0.6245\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 0.3107 - acc: 0.9410 - val_loss: 1.6498 - val_acc: 0.6624\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 11s 281us/step - loss: 0.1415 - acc: 0.9824 - val_loss: 1.4760 - val_acc: 0.6989\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 11s 281us/step - loss: 0.0556 - acc: 0.9966 - val_loss: 1.5022 - val_acc: 0.7024\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 0.0298 - acc: 0.9988 - val_loss: 1.4815 - val_acc: 0.7114\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 0.0160 - acc: 0.9995 - val_loss: 1.5102 - val_acc: 0.7084\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 12s 310us/step - loss: 3.4223 - acc: 0.2150 - val_loss: 3.1826 - val_acc: 0.2422\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 11s 278us/step - loss: 2.3119 - acc: 0.4389 - val_loss: 3.5490 - val_acc: 0.2347\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 11s 278us/step - loss: 1.5782 - acc: 0.6181 - val_loss: 2.8839 - val_acc: 0.3726\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 11s 280us/step - loss: 0.9956 - acc: 0.7653 - val_loss: 1.7672 - val_acc: 0.5860\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 11s 277us/step - loss: 0.5668 - acc: 0.8764 - val_loss: 1.7269 - val_acc: 0.6282\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 11s 276us/step - loss: 0.2869 - acc: 0.9493 - val_loss: 1.6630 - val_acc: 0.6520\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 11s 277us/step - loss: 0.1334 - acc: 0.9836 - val_loss: 1.5831 - val_acc: 0.6936\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 11s 278us/step - loss: 0.0558 - acc: 0.9962 - val_loss: 1.5518 - val_acc: 0.7001\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 11s 275us/step - loss: 0.0243 - acc: 0.9991 - val_loss: 1.5059 - val_acc: 0.7106\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 11s 273us/step - loss: 0.0155 - acc: 0.9995 - val_loss: 1.5162 - val_acc: 0.7104\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 12s 301us/step - loss: 3.4492 - acc: 0.2094 - val_loss: 3.5958 - val_acc: 0.1626\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 2.3568 - acc: 0.4259 - val_loss: 3.0267 - val_acc: 0.2905\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 1.6278 - acc: 0.6074 - val_loss: 2.2439 - val_acc: 0.4423\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 1.0436 - acc: 0.7547 - val_loss: 1.9531 - val_acc: 0.5268\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 11s 285us/step - loss: 0.6064 - acc: 0.8663 - val_loss: 1.8345 - val_acc: 0.5881\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 12s 290us/step - loss: 0.3111 - acc: 0.9409 - val_loss: 1.5516 - val_acc: 0.6775\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 12s 292us/step - loss: 0.1364 - acc: 0.9838 - val_loss: 1.5661 - val_acc: 0.6873\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 12s 294us/step - loss: 0.0583 - acc: 0.9962 - val_loss: 1.4978 - val_acc: 0.7012\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 12s 294us/step - loss: 0.0277 - acc: 0.9986 - val_loss: 1.5124 - val_acc: 0.7035\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 12s 291us/step - loss: 0.0149 - acc: 0.9997 - val_loss: 1.5161 - val_acc: 0.7084\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 12s 308us/step - loss: 3.4663 - acc: 0.2082 - val_loss: 3.3228 - val_acc: 0.2246\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 2.3401 - acc: 0.4319 - val_loss: 2.5393 - val_acc: 0.3724\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 11s 285us/step - loss: 1.5964 - acc: 0.6135 - val_loss: 2.1334 - val_acc: 0.4798\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 0.9908 - acc: 0.7688 - val_loss: 2.1010 - val_acc: 0.5164\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 11s 284us/step - loss: 0.5666 - acc: 0.8765 - val_loss: 1.6376 - val_acc: 0.6464\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 11s 286us/step - loss: 0.2778 - acc: 0.9501 - val_loss: 1.8175 - val_acc: 0.6280\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 11s 282us/step - loss: 0.1239 - acc: 0.9861 - val_loss: 1.5855 - val_acc: 0.6828\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 0.0521 - acc: 0.9965 - val_loss: 1.5468 - val_acc: 0.6938\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 0.0269 - acc: 0.9990 - val_loss: 1.5629 - val_acc: 0.6984\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 0.0166 - acc: 0.9992 - val_loss: 1.5913 - val_acc: 0.6990\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 12s 303us/step - loss: 3.3757 - acc: 0.2268 - val_loss: 3.3788 - val_acc: 0.2393\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 12s 291us/step - loss: 2.2513 - acc: 0.4547 - val_loss: 2.9639 - val_acc: 0.3317\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 1.5131 - acc: 0.6372 - val_loss: 1.9880 - val_acc: 0.5127\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 12s 292us/step - loss: 0.9338 - acc: 0.7830 - val_loss: 1.9250 - val_acc: 0.5589\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 12s 294us/step - loss: 0.5235 - acc: 0.8880 - val_loss: 1.7351 - val_acc: 0.6337\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 0.2587 - acc: 0.9542 - val_loss: 1.5345 - val_acc: 0.6834\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.1138 - acc: 0.9871 - val_loss: 1.5124 - val_acc: 0.7009\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 12s 290us/step - loss: 0.0540 - acc: 0.9963 - val_loss: 1.5572 - val_acc: 0.7026\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 11s 286us/step - loss: 0.0246 - acc: 0.9989 - val_loss: 1.5354 - val_acc: 0.7065\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 0.0166 - acc: 0.9994 - val_loss: 1.5433 - val_acc: 0.7092\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 12s 310us/step - loss: 3.4596 - acc: 0.2081 - val_loss: 3.4560 - val_acc: 0.1975\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 2.3322 - acc: 0.4345 - val_loss: 3.6012 - val_acc: 0.2180\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 12s 292us/step - loss: 1.5869 - acc: 0.6173 - val_loss: 2.7812 - val_acc: 0.3787\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 12s 297us/step - loss: 1.0046 - acc: 0.7654 - val_loss: 1.8973 - val_acc: 0.5595\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 12s 292us/step - loss: 0.5684 - acc: 0.8760 - val_loss: 1.6064 - val_acc: 0.6449\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 11s 285us/step - loss: 0.2867 - acc: 0.9476 - val_loss: 1.5171 - val_acc: 0.6791\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 11s 285us/step - loss: 0.1247 - acc: 0.9854 - val_loss: 1.5268 - val_acc: 0.6987\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 11s 286us/step - loss: 0.0524 - acc: 0.9967 - val_loss: 1.5500 - val_acc: 0.7040\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.0270 - acc: 0.9987 - val_loss: 1.5817 - val_acc: 0.7008\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 0.0168 - acc: 0.9995 - val_loss: 1.5428 - val_acc: 0.7066\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 12s 310us/step - loss: 3.4079 - acc: 0.2167 - val_loss: 3.6142 - val_acc: 0.1631\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 12s 293us/step - loss: 2.3119 - acc: 0.4350 - val_loss: 2.6524 - val_acc: 0.3464\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 12s 293us/step - loss: 1.5866 - acc: 0.6149 - val_loss: 3.2005 - val_acc: 0.3104\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 12s 292us/step - loss: 1.0157 - acc: 0.7621 - val_loss: 2.2529 - val_acc: 0.4895\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.5850 - acc: 0.8704 - val_loss: 1.8162 - val_acc: 0.5969\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 12s 291us/step - loss: 0.2934 - acc: 0.9462 - val_loss: 1.6493 - val_acc: 0.6654\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 12s 291us/step - loss: 0.1308 - acc: 0.9846 - val_loss: 1.5618 - val_acc: 0.6820\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 0.0610 - acc: 0.9953 - val_loss: 1.5246 - val_acc: 0.7041\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 12s 290us/step - loss: 0.0279 - acc: 0.9987 - val_loss: 1.5147 - val_acc: 0.7096\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 12s 291us/step - loss: 0.0152 - acc: 0.9996 - val_loss: 1.5136 - val_acc: 0.7090\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 13s 315us/step - loss: 3.4206 - acc: 0.2113 - val_loss: 3.2607 - val_acc: 0.2350\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 11s 283us/step - loss: 2.3185 - acc: 0.4385 - val_loss: 3.0504 - val_acc: 0.2706\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 11s 285us/step - loss: 1.5776 - acc: 0.6220 - val_loss: 2.1114 - val_acc: 0.4905\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 11s 286us/step - loss: 0.9950 - acc: 0.7679 - val_loss: 1.8468 - val_acc: 0.5655\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.5610 - acc: 0.8780 - val_loss: 1.7361 - val_acc: 0.6138\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.2878 - acc: 0.9471 - val_loss: 1.5411 - val_acc: 0.6793\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.1303 - acc: 0.9839 - val_loss: 1.5569 - val_acc: 0.6840\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.0544 - acc: 0.9962 - val_loss: 1.5559 - val_acc: 0.6985\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 0.0261 - acc: 0.9987 - val_loss: 1.4796 - val_acc: 0.7139\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 12s 293us/step - loss: 0.0148 - acc: 0.9996 - val_loss: 1.4939 - val_acc: 0.7138\n"
     ]
    }
   ],
   "source": [
    "# Train bagged model\n",
    "base_model = base_learner_bagging(model_input, 3)\n",
    "print(base_model.summary())\n",
    "\n",
    "models = []\n",
    "for i in np.arange(40,50):\n",
    "    base_model = base_learner_bagging(model_input, i)\n",
    "    _ = compile_and_train_bagging(base_model, 10, x_train, y_train)\n",
    "    models.append(base_model)\n",
    "    base_model.save_weights('weights_CIFAR100/weights_CIFAR100_base_learner_' + str(i) + '_bagging.hdf5')\n",
    "    del base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 13s 335us/step - loss: 3.5066 - acc: 0.1961 - val_loss: 3.4549 - val_acc: 0.1899\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 2.5594 - acc: 0.3678 - val_loss: 2.9144 - val_acc: 0.3037\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 2.0668 - acc: 0.4753 - val_loss: 2.8751 - val_acc: 0.3190\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 1.6615 - acc: 0.5690 - val_loss: 2.8513 - val_acc: 0.3407\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 1.2736 - acc: 0.6701 - val_loss: 2.6704 - val_acc: 0.3680\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.9077 - acc: 0.7697 - val_loss: 2.8693 - val_acc: 0.3623\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.5882 - acc: 0.8594 - val_loss: 2.9672 - val_acc: 0.3601\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 0.3456 - acc: 0.9316 - val_loss: 3.0982 - val_acc: 0.3619\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 0.1914 - acc: 0.9713 - val_loss: 3.0529 - val_acc: 0.3815\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.1002 - acc: 0.9894 - val_loss: 3.2099 - val_acc: 0.3763\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.0518 - acc: 0.9965 - val_loss: 3.2715 - val_acc: 0.3866\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.0309 - acc: 0.9982 - val_loss: 3.3477 - val_acc: 0.3871\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.0198 - acc: 0.9992 - val_loss: 3.4034 - val_acc: 0.3888\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 0.0167 - acc: 0.9992 - val_loss: 3.3573 - val_acc: 0.3854\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 0.0136 - acc: 0.9993 - val_loss: 3.4516 - val_acc: 0.3884\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 11s 287us/step - loss: 0.0122 - acc: 0.9995 - val_loss: 3.5943 - val_acc: 0.3826\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 12s 289us/step - loss: 0.0118 - acc: 0.9995 - val_loss: 3.5189 - val_acc: 0.3842\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.0239 - acc: 0.9975 - val_loss: 3.8415 - val_acc: 0.3629\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.0286 - acc: 0.9972 - val_loss: 4.0683 - val_acc: 0.3450\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 12s 288us/step - loss: 0.5387 - acc: 0.8421 - val_loss: 6.4145 - val_acc: 0.2474\n"
     ]
    }
   ],
   "source": [
    "# Train single model with full dataset\n",
    "base_model_full = base_learner_bagging(model_input, 50)\n",
    "_ = compile_and_train(base_model_full, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bagged model (whole models)\n",
    "\n",
    "models = []\n",
    "for i in np.arange(0,50):\n",
    "    models.append(load_model('models/CIFAR100_base_learner_' + str(i) + '_bagging.h5'))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# Load bagged model (only weights)\n",
    "\n",
    "models_new = []\n",
    "for i in np.arange(0,50):\n",
    "    base_model = base_learner_bagging(model_input, i)\n",
    "    base_model.load_weights('weights_CIFAR100/weights_CIFAR100_base_learner_' + str(i) + '_bagging.hdf5')\n",
    "    models_new.append(base_model)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 30, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 13, 13, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 13, 13, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 755,556\n",
      "Trainable params: 0\n",
      "Non-trainable params: 755,556\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 30, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 13, 13, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 13, 13, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 755,556\n",
      "Trainable params: 0\n",
      "Non-trainable params: 755,556\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Freeze weights in bagged model\n",
    "\n",
    "model_0 = models_new[2]\n",
    "model_0.summary()\n",
    "\n",
    "for model in models_new:\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the different magged models\n",
    "\n",
    "#base_model = base_learner_bagging(model_input, 3)\n",
    "#base_model.load_weights('weights/CIFAR10/CIFAR10_base_learner_3_bagging.17-0.75.hdf5')\n",
    "for model in models:\n",
    "        print(evaluate_test_accuracy(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.397\n"
     ]
    }
   ],
   "source": [
    "# Evaluate single model with full dataset\n",
    "base_model = base_learner_bagging(model_input, 50)\n",
    "base_model.load_weights('weights/CIFAR100/CIFAR100_base_learner_50_bagging.13-0.39.hdf5')\n",
    "print(evaluate_test_accuracy(base_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate simple combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "2 test_acc: [0.3434]\n",
      "(3,)\n",
      "3 test_acc: [0.3872]\n",
      "(4,)\n",
      "4 test_acc: [0.4119]\n",
      "(5,)\n",
      "5 test_acc: [0.431]\n",
      "(6,)\n",
      "6 test_acc: [0.4435]\n",
      "(7,)\n",
      "7 test_acc: [0.4508]\n",
      "(8,)\n",
      "8 test_acc: [0.4607]\n",
      "(9,)\n",
      "9 test_acc: [0.4671]\n",
      "(10,)\n",
      "10 test_acc: [0.4742]\n",
      "(11,)\n",
      "11 test_acc: [0.4766]\n",
      "(12,)\n",
      "12 test_acc: [0.4795]\n",
      "(13,)\n",
      "13 test_acc: [0.4849]\n",
      "(14,)\n",
      "14 test_acc: [0.4864]\n",
      "(15,)\n",
      "15 test_acc: [0.4878]\n",
      "(16,)\n",
      "16 test_acc: [0.4905]\n",
      "(17,)\n",
      "17 test_acc: [0.4927]\n",
      "(18,)\n",
      "18 test_acc: [0.4938]\n",
      "(19,)\n",
      "19 test_acc: [0.4928]\n",
      "(20,)\n",
      "20 test_acc: [0.4957]\n",
      "(21,)\n",
      "21 test_acc: [0.4971]\n",
      "(22,)\n",
      "22 test_acc: [0.4989]\n",
      "(23,)\n",
      "23 test_acc: [0.4987]\n",
      "(24,)\n",
      "24 test_acc: [0.5001]\n",
      "(25,)\n",
      "25 test_acc: [0.503]\n",
      "(26,)\n",
      "26 test_acc: [0.5028]\n",
      "(27,)\n",
      "27 test_acc: [0.5042]\n",
      "(28,)\n",
      "28 test_acc: [0.5049]\n",
      "(29,)\n",
      "29 test_acc: [0.506]\n",
      "(30,)\n",
      "30 test_acc: [0.5057]\n",
      "(31,)\n",
      "31 test_acc: [0.5079]\n",
      "(32,)\n",
      "32 test_acc: [0.5064]\n",
      "(33,)\n",
      "33 test_acc: [0.5062]\n",
      "(34,)\n",
      "34 test_acc: [0.5059]\n",
      "(35,)\n",
      "35 test_acc: [0.5065]\n",
      "(36,)\n",
      "36 test_acc: [0.5081]\n",
      "(37,)\n",
      "37 test_acc: [0.5087]\n",
      "(38,)\n",
      "38 test_acc: [0.5093]\n",
      "(39,)\n",
      "39 test_acc: [0.5106]\n",
      "(40,)\n",
      "40 test_acc: [0.5116]\n",
      "(41,)\n",
      "41 test_acc: [0.5094]\n",
      "(42,)\n",
      "42 test_acc: [0.5119]\n",
      "(43,)\n",
      "43 test_acc: [0.5116]\n",
      "(44,)\n",
      "44 test_acc: [0.5114]\n",
      "(45,)\n",
      "45 test_acc: [0.512]\n",
      "(46,)\n",
      "46 test_acc: [0.5135]\n",
      "(47,)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_139/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, _class=[\"loc:@batch_normalization_139/cond/Switch_1\"], data_format=\"NCHW\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_93/Relu, batch_normalization_139/gamma/read, batch_normalization_139/beta/read, batch_normalization_1/Const_4, batch_normalization_1/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: concatenate_46/concat/_12075 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8152_concatenate_46/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2e6020ecda60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpred_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmax_voting_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_voting_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_139/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, _class=[\"loc:@batch_normalization_139/cond/Switch_1\"], data_format=\"NCHW\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_93/Relu, batch_normalization_139/gamma/read, batch_normalization_139/beta/read, batch_normalization_1/Const_4, batch_normalization_1/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: concatenate_46/concat/_12075 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8152_concatenate_46/concat\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# Max voting\n",
    "\n",
    "max_voting_accuracies = np.zeros((51,1))\n",
    "for k in np.arange(2,50):\n",
    "    n_models = k\n",
    "    models_short = models_new[0:n_models]\n",
    "\n",
    "    pred_new = np.zeros((10000,100))\n",
    "    max_voting_model = concatenate_ensemble(models_short, model_input)\n",
    "    pred = max_voting_model.predict(x_test, batch_size = 256)\n",
    "    pred = np.expand_dims(pred, axis=1)\n",
    "    for i in range(n_models):\n",
    "        j = i*100\n",
    "        #print(i)\n",
    "        #print(np.shape(np.array(pred)))\n",
    "        #print(np.shape(np.array(pred)))\n",
    "        pred_part = pred[:,:,j:j+100]\n",
    "        #print(np.shape(np.array(pred)))\n",
    "        index = np.argmax(pred_part, axis=2)\n",
    "        #print(np.shape(np.array(index)))\n",
    "        index = to_categorical(index, num_classes=100)\n",
    "        #print(np.shape(np.array(index)))\n",
    "        #print(index[i,:])\n",
    "        pred_new += index\n",
    "        #print(pred_new[0:3,:])\n",
    "\n",
    "    pred_new = np.expand_dims(pred_new, axis=1)\n",
    "    pred_new = np.argmax(pred_new, axis=2)\n",
    "    max_voting_accuracies[k] = np.sum(np.equal(pred_new, y_test)) / y_test.shape[0]\n",
    "    print(k,\"test_acc:\", max_voting_accuracies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('CIFAR100_max_voting_accuracies.txt', max_voting_accuracies, fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('CIFAR100_average_accuracies.txt', average_accuracies, fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "2 test_acc: [0.3939]\n",
      "(3,)\n",
      "3 test_acc: [0.4267]\n",
      "(4,)\n",
      "4 test_acc: [0.442]\n",
      "(5,)\n",
      "5 test_acc: [0.4589]\n",
      "(6,)\n",
      "6 test_acc: [0.4681]\n",
      "(7,)\n",
      "7 test_acc: [0.4722]\n",
      "(8,)\n",
      "8 test_acc: [0.4801]\n",
      "(9,)\n",
      "9 test_acc: [0.4847]\n",
      "(10,)\n",
      "10 test_acc: [0.4901]\n",
      "(11,)\n",
      "11 test_acc: [0.4928]\n",
      "(12,)\n",
      "12 test_acc: [0.4972]\n",
      "(13,)\n",
      "13 test_acc: [0.498]\n",
      "(14,)\n",
      "14 test_acc: [0.4997]\n",
      "(15,)\n",
      "15 test_acc: [0.5012]\n",
      "(16,)\n",
      "16 test_acc: [0.5016]\n",
      "(17,)\n",
      "17 test_acc: [0.504]\n",
      "(18,)\n",
      "18 test_acc: [0.503]\n",
      "(19,)\n",
      "19 test_acc: [0.502]\n",
      "(20,)\n",
      "20 test_acc: [0.5054]\n",
      "(21,)\n",
      "21 test_acc: [0.507]\n",
      "(22,)\n",
      "22 test_acc: [0.5087]\n",
      "(23,)\n",
      "23 test_acc: [0.508]\n",
      "(24,)\n",
      "24 test_acc: [0.5083]\n",
      "(25,)\n",
      "25 test_acc: [0.5094]\n",
      "(26,)\n",
      "26 test_acc: [0.5091]\n",
      "(27,)\n",
      "27 test_acc: [0.5103]\n",
      "(28,)\n",
      "28 test_acc: [0.5108]\n",
      "(29,)\n",
      "29 test_acc: [0.5122]\n",
      "(30,)\n",
      "30 test_acc: [0.5104]\n",
      "(31,)\n",
      "31 test_acc: [0.5107]\n",
      "(32,)\n",
      "32 test_acc: [0.5108]\n",
      "(33,)\n",
      "33 test_acc: [0.5109]\n",
      "(34,)\n",
      "34 test_acc: [0.512]\n",
      "(35,)\n",
      "35 test_acc: [0.5121]\n",
      "(36,)\n",
      "36 test_acc: [0.512]\n",
      "(37,)\n",
      "37 test_acc: [0.5119]\n",
      "(38,)\n",
      "38 test_acc: [0.5128]\n",
      "(39,)\n",
      "39 test_acc: [0.5134]\n",
      "(40,)\n",
      "40 test_acc: [0.5142]\n",
      "(41,)\n",
      "41 test_acc: [0.5139]\n",
      "(42,)\n",
      "42 test_acc: [0.5155]\n",
      "(43,)\n",
      "43 test_acc: [0.5168]\n",
      "(44,)\n",
      "44 test_acc: [0.5161]\n",
      "(45,)\n",
      "45 test_acc: [0.5167]\n",
      "(46,)\n",
      "46 test_acc: [0.516]\n",
      "(47,)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_139/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, _class=[\"loc:@batch_normalization_139/cond/Switch_1\"], data_format=\"NCHW\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_93/Relu, batch_normalization_139/gamma/read, batch_normalization_139/beta/read, batch_normalization_1/Const_4, batch_normalization_1/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: average_46/truediv/_12075 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8198_average_46/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-44a5f151ef08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodels_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maverage_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0maverage_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test_acc:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1162763e6a3f>\u001b[0m in \u001b[0;36mevaluate_test_accuracy\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make same shape as y_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: batch_normalization_139/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, _class=[\"loc:@batch_normalization_139/cond/Switch_1\"], data_format=\"NCHW\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_93/Relu, batch_normalization_139/gamma/read, batch_normalization_139/beta/read, batch_normalization_1/Const_4, batch_normalization_1/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: average_46/truediv/_12075 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_8198_average_46/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# Average\n",
    "\n",
    "average_accuracies = np.zeros((51,1))\n",
    "#for model in models_new:\n",
    "#    print(model.name)\n",
    "#    model.save_weights('weights_CIFAR10/weights_' + model.name +'.hdf5')\n",
    "\n",
    "for i in np.arange(2,50):\n",
    "    n_models = i\n",
    "    #print(n_models)\n",
    "    models_short = models_new[0:n_models]\n",
    "    average_model = average_ensemble(models_short, model_input)\n",
    "    average_accuracies[i] = evaluate_test_accuracy(average_model)\n",
    "    print(i,\"test_acc:\", average_accuracies[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train stacking combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('CIFAR100_basic_stacking_accuracies.txt', basic_stacking_accuracies, fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 3.5595 - acc: 0.7187 - val_loss: 1.6440 - val_acc: 0.9764\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 60s 1ms/step - loss: 0.6165 - acc: 0.9894 - val_loss: 0.2347 - val_acc: 0.9910\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 60s 2ms/step - loss: 0.1448 - acc: 0.9938 - val_loss: 0.1136 - val_acc: 0.9909\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 60s 1ms/step - loss: 0.0789 - acc: 0.9944 - val_loss: 0.0783 - val_acc: 0.9912\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 60s 1ms/step - loss: 0.0548 - acc: 0.9951 - val_loss: 0.0624 - val_acc: 0.9913\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 60s 1ms/step - loss: 0.0418 - acc: 0.9954 - val_loss: 0.0538 - val_acc: 0.9911\n",
      "Epoch 7/10\n",
      " 2560/40000 [>.............................] - ETA: 43s - loss: 0.0311 - acc: 0.9973"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ffbd854ecc90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mensemble_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmall_stacking_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_acc:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1162763e6a3f>\u001b[0m in \u001b[0;36mcompile_and_train\u001b[0;34m(model, num_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtensor_board\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_board\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train single ensemble\n",
    "\n",
    "n_models = 15\n",
    "ensemble_model = small_stacking_ensemble(models_new[0:n_models], model_input, n_models)\n",
    "_ = compile_and_train(ensemble_model, num_epochs=10)\n",
    "accuracy = evaluate_test_accuracy(ensemble_model)\n",
    "print(n_models, \"test_acc:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4863\n"
     ]
    }
   ],
   "source": [
    "# Evaluate single ensemble\n",
    "ensemble_model = small_stacking_ensemble(models_new[0:n_models], model_input, n_models)\n",
    "ensemble_model.load_weights('weights/CIFAR100/CIFAR100_small_stacking_15_ensemble.05-0.99.hdf5')\n",
    "print(evaluate_test_accuracy(ensemble_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 12s 290us/step - loss: 4.4440 - acc: 0.5019 - val_loss: 4.2885 - val_acc: 0.7864\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 10s 249us/step - loss: 4.1298 - acc: 0.8396 - val_loss: 3.9930 - val_acc: 0.8390\n",
      "2 test_acc: [0.3957]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 22s 558us/step - loss: 4.2051 - acc: 0.7007 - val_loss: 3.8214 - val_acc: 0.9314\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 21s 526us/step - loss: 3.4483 - acc: 0.9526 - val_loss: 3.1300 - val_acc: 0.9449\n",
      "5 test_acc: [0.4589]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 41s 1ms/step - loss: 3.8227 - acc: 0.8084 - val_loss: 3.0994 - val_acc: 0.9756\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 40s 997us/step - loss: 2.4511 - acc: 0.9867 - val_loss: 1.9552 - val_acc: 0.9799\n",
      "10 test_acc: [0.4893]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 3.4506 - acc: 0.8551 - val_loss: 2.4332 - val_acc: 0.9881\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 59s 1ms/step - loss: 1.6680 - acc: 0.9936 - val_loss: 1.1793 - val_acc: 0.9901\n",
      "15 test_acc: [0.4984]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 81s 2ms/step - loss: 3.1331 - acc: 0.8710 - val_loss: 1.8894 - val_acc: 0.9928\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 78s 2ms/step - loss: 1.1456 - acc: 0.9964 - val_loss: 0.7501 - val_acc: 0.9947\n",
      "20 test_acc: [0.5024]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 101s 3ms/step - loss: 2.8288 - acc: 0.8811 - val_loss: 1.4370 - val_acc: 0.9946\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 98s 2ms/step - loss: 0.7936 - acc: 0.9977 - val_loss: 0.5049 - val_acc: 0.9965\n",
      "25 test_acc: [0.5086]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 2.5348 - acc: 0.9054 - val_loss: 1.0801 - val_acc: 0.9972\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 117s 3ms/step - loss: 0.5604 - acc: 0.9981 - val_loss: 0.3575 - val_acc: 0.9972\n",
      "30 test_acc: [0.5095]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 2.2938 - acc: 0.9125 - val_loss: 0.8275 - val_acc: 0.9968\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.4126 - acc: 0.9988 - val_loss: 0.2690 - val_acc: 0.9976\n",
      "35 test_acc: [0.5135]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 2.1042 - acc: 0.9168 - val_loss: 0.6501 - val_acc: 0.9977\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 0.3160 - acc: 0.9989 - val_loss: 0.2105 - val_acc: 0.9983\n",
      "40 test_acc: [0.5149]\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 184s 5ms/step - loss: 1.9312 - acc: 0.9177 - val_loss: 0.5156 - val_acc: 0.9980\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 177s 4ms/step - loss: 0.2470 - acc: 0.9991 - val_loss: 0.1694 - val_acc: 0.9982\n",
      "45 test_acc: [0.5175]\n"
     ]
    }
   ],
   "source": [
    "# Train ensemble\n",
    "\n",
    "basic_stacking_accuracies = np.zeros((51,1))\n",
    "\n",
    "range_list = np.arange(0,50,5)\n",
    "range_list[0] = 2\n",
    "\n",
    "for i in range_list:\n",
    "    n_models = i\n",
    "    #print(n_models)\n",
    "    #models_short = models_new[0:n_models]\n",
    "    ensemble_model = basic_stacking_ensemble(models_new[0:n_models], model_input, i)\n",
    "    _ = compile_and_train(ensemble_model, num_epochs=2)\n",
    "    basic_stacking_accuracies[i] = evaluate_test_accuracy(ensemble_model)\n",
    "    print(i,\"test_acc:\", basic_stacking_accuracies[i])\n",
    "\n",
    "#n_models = 25\n",
    "\n",
    "#ensemble_model = basic_stacking_ensemble(models_new[0:n_models], model_input)\n",
    "#print(ensemble_model.summary())\n",
    "#_ = compile_and_train(ensemble_model, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble\n",
    "\n",
    "ensemble_model = basic_stacking_ensemble(models_new[0:n_models], model_input)\n",
    "ensemble_model.load_weights('weights/CIFAR10/CIFAR10_basic_stacking_ensemble.01-0.99.hdf5')\n",
    "#print(ensemble_model.summary())\n",
    "print(evaluate_test_accuracy(ensemble_model))\n",
    "\n",
    "#for model in models:\n",
    "#        print(evaluate_test_accuracy(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
