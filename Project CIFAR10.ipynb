{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning for image classification (CIFAR10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"36\"\n",
    "NAME1 = \"Timothy Hellberg\"\n",
    "NAME2 = \"Lars Liberg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# For dealing with files\n",
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "# For using regex expressions\n",
    "import re\n",
    "\n",
    "# For splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Packages for defining the architecture of our model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, BatchNormalization, Input, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import load_model \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# For generating data\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# One-hot encoding\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Callbacks for training\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "# Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ndarray computations\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix for assessment step\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# Other\n",
    "import pandas as pd\n",
    "from subprocess import check_output\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as _Imgdis\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from time import sleep\n",
    "from scipy import ndimage\n",
    "from scipy.misc import imresize\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Average, Dropout, Maximum\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Concatenate\n",
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#home_path = \"/Users/timhell/Google Drive/11 FaÌˆrdigheter/Programmering/Python/Deep machine learning/deep-machine-learning/Project\"\n",
    "home_path = \"/home/student/deep-machine-learning/Project\"\n",
    "os.chdir(home_path)\n",
    "\n",
    "# Dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "print('x_train shape: {} | y_train shape: {}\\nx_test shape : {} | y_test shape : {}'.format(x_train.shape, y_train.shape,                                                                \n",
    "                                                                                            x_test.shape, y_test.shape))\n",
    "input_shape = x_train[0,:,:,:].shape\n",
    "print(\"input_shape:\",input_shape)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "#plt.imshow(x_train[np.random.randint(50000)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Functions, learners and combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal functions\n",
    "\n",
    "def compile_and_train(model, num_epochs): \n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['acc']) \n",
    "    filepath = home_path + '/weights/CIFAR10/' + model.name + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_weights_only=True, save_best_only=True, mode='auto', period=1)\n",
    "    tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=32)\n",
    "    history = model.fit(x=x_train, y=y_train, batch_size=256, epochs=num_epochs, verbose=1, callbacks=[checkpoint, tensor_board], validation_split=0.2)\n",
    "    return history\n",
    "\n",
    "def compile_and_train_bagging(model, num_epochs, x_train, y_train): \n",
    "    indices = np.random.randint(50000, size=50000)\n",
    "    x_train_bag = x_train[indices,:,:,:]\n",
    "    y_train_bag = y_train[indices,:]\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['acc']) \n",
    "    filepath = home_path + '/weights/CIFAR10/' + model.name + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_weights_only=True, save_best_only=True, mode='auto', period=1)\n",
    "    tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=32)\n",
    "    history = model.fit(x=x_train_bag, y=y_train_bag, batch_size=256, epochs=num_epochs, verbose=1, callbacks=[checkpoint, tensor_board], validation_split=0.2)\n",
    "    del x_train_bag, y_train_bag\n",
    "    return history\n",
    "\n",
    "def evaluate_test_accuracy(model):\n",
    "    pred = model.predict(x_test, batch_size = 256)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    pred = np.expand_dims(pred, axis=1) # make same shape as y_test\n",
    "    test_accuracy = np.sum(np.equal(pred, y_test)) / y_test.shape[0]  \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom base learners\n",
    "\n",
    "def base_cnn_reg(model_input):\n",
    "    x = Conv2D(128, (5, 5), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR10_base_cnn_reg')\n",
    "    return model\n",
    "\n",
    "def base_cnn_2_reg(model_input): # same as the bagged model\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR10_base_cnn_2_reg')\n",
    "    return model\n",
    "\n",
    "def base_cnn_2_reg_fewer_filters_and_dense_neurons(model_input):\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR10_base_cnn_2_reg_fewer_filters_and_dense_neurons')\n",
    "    return model\n",
    "\n",
    "def base_cnn_3_reg(model_input):\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR10_base_cnn_3_reg')\n",
    "    return model\n",
    "\n",
    "def base_cnn_4_conv_concise(model_input):\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR10_base_cnn_4_conv_concise')\n",
    "    return model\n",
    "\n",
    "def base_learner_bagging(model_input, num):\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(10, activation='softmax')(x)\n",
    "    model = Model(model_input, x, name='CIFAR10_base_learner_' + str(num) + '_bagging')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom combiners\n",
    "\n",
    "def average_ensemble(models, model_input):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    #print(np.shape(np.array(outputs)))\n",
    "    #print(outputs[0])\n",
    "    y = Average()(outputs)\n",
    "    model = Model(models[0].input, y, name='CIFAR10_average_ensemble')\n",
    "    return model\n",
    "\n",
    "def concatenate_ensemble(models, model_input):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    print(np.shape(np.array(outputs)))\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    model = Model(model_input, y, name='CIFAR10_concatenate_ensemble')\n",
    "    return model\n",
    "\n",
    "def basic_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(10, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR10_basic_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def small_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(10, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR10_small_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def large_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(200, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(10, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR10_large_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def deep_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(10, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR10_deep_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def bn_deep_stacking_ensemble(models, model_input, num):\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = BatchNormalization()(y)    \n",
    "    y = Dense(10, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR10_bn_deep_stacking_' + str(num) + '_ensemble')\n",
    "    return model\n",
    "\n",
    "def pop_stacking_ensemble(models, model_input, num):\n",
    "    for model in models:\n",
    "        model.layers.pop()\n",
    "    outputs = [model.outputs[0] for model in models]\n",
    "    y = Concatenate(axis=-1)(outputs)\n",
    "    y = Dense(20, activation='relu',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    y = Dense(10, activation='softmax',\n",
    "              kernel_initializer='random_uniform', \n",
    "              bias_initializer='random_uniform')(y)\n",
    "    model = Model(model_input, y, name='CIFAR10_pop_stacking_' + str(num) + '_ensemble')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train base learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Bagging (train base learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train bagged model\n",
    "base_model = base_learner_bagging(model_input, 3)\n",
    "print(base_model.summary())\n",
    "\n",
    "models = []\n",
    "for i in np.arange(0,50):\n",
    "    base_model = base_learner_bagging(model_input, i)\n",
    "    _ = compile_and_train_bagging(base_model, 10, x_train, y_train)\n",
    "    models.append(base_model)\n",
    "    base_model.save_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + str(i) + '_bagging.hdf5')\n",
    "    del base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train single model with full dataset\n",
    "base_model_full = base_learner_bagging(model_input, 50)\n",
    "print(base_model_full.summary())\n",
    "_ = compile_and_train(base_model_full, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bagged model (only weights)\n",
    "\n",
    "models_bag = []\n",
    "for i in np.arange(0,50):\n",
    "    base_model = base_learner_bagging(model_input, i)\n",
    "    base_model.load_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + str(i) + '_bagging.hdf5')\n",
    "    models_bag.append(base_model)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze weights in bagged model\n",
    "\n",
    "model_0 = models_bag[2]\n",
    "model_0.summary()\n",
    "\n",
    "for model in models_bag:\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the different bagged models\n",
    "\n",
    "for model in models_bag:\n",
    "    print(evaluate_test_accuracy(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Non-bagging (train base learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "\n",
    "\n",
    "base_model = base_cnn_reg(model_input)\n",
    "_ = compile_and_train(base_model, 20)\n",
    "base_model.save_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "\n",
    "base_model = base_cnn_2_reg(model_input)\n",
    "_ = compile_and_train(base_model, 20)\n",
    "base_model.save_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "\n",
    "base_model = base_cnn_2_reg_fewer_filters_and_dense_neurons(model_input)\n",
    "_ = compile_and_train(base_model, 20)\n",
    "base_model.save_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "\n",
    "base_model = base_cnn_3_reg(model_input)\n",
    "_ = compile_and_train(base_model, 20)\n",
    "base_model.save_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "\n",
    "base_model = base_cnn_4_conv_concise(model_input)\n",
    "_ = compile_and_train(base_model, 20)\n",
    "base_model.save_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train single model\n",
    "\n",
    "base_model = base_cnn_reg(model_input)\n",
    "_ = compile_and_train(base_model, 10)\n",
    "base_model.save_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model without saving weights\n",
    "\n",
    "base_model = base_cnn(model_input)\n",
    "print(base_model.summary())\n",
    "\n",
    "_ = compile_and_train(base_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models (only weights)\n",
    "\n",
    "models_nonbag = []\n",
    "\n",
    "base_model = base_cnn_reg(model_input)\n",
    "base_model.load_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "models_nonbag.append(base_model)\n",
    "\n",
    "base_model = base_cnn_2_reg(model_input)\n",
    "base_model.load_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "models_nonbag.append(base_model)\n",
    "\n",
    "base_model = base_cnn_2_reg_fewer_filters_and_dense_neurons(model_input)\n",
    "base_model.load_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "models_nonbag.append(base_model)\n",
    "\n",
    "base_model = base_cnn_3_reg(model_input)\n",
    "base_model.load_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "models_nonbag.append(base_model)\n",
    "\n",
    "base_model = base_cnn_4_conv_concise(model_input)\n",
    "base_model.load_weights('weights_CIFAR10/weights_CIFAR10_base_learner_' + base_model.name + '_non_bagging.hdf5')\n",
    "models_nonbag.append(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze weights in non-bagged model\n",
    "\n",
    "model_0 = models_nonbag[0]\n",
    "model_0.summary()\n",
    "\n",
    "for model in models_nonbag:\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the different non-bagged models\n",
    "\n",
    "for model in models_nonbag:\n",
    "    print(evaluate_test_accuracy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate simple combiners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Bagging (evaluate simple combiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max voting\n",
    "\n",
    "max_voting_accuracies = np.zeros((51,1))\n",
    "for k in np.arange(2,50):\n",
    "    n_models = k\n",
    "    models_short = models_bag[0:n_models]\n",
    "\n",
    "    pred_new = np.zeros((10000,10))\n",
    "    max_voting_model = concatenate_ensemble(models_short, model_input)\n",
    "    pred = max_voting_model.predict(x_test, batch_size = 256)\n",
    "    pred = np.expand_dims(pred, axis=1)\n",
    "    for i in range(n_models):\n",
    "        j = i*10\n",
    "        #print(i)\n",
    "        #print(np.shape(np.array(pred)))\n",
    "        #print(np.shape(np.array(pred)))\n",
    "        pred_part = pred[:,:,j:j+10]\n",
    "        #print(np.shape(np.array(pred)))\n",
    "        index = np.argmax(pred_part, axis=2)\n",
    "        #print(np.shape(np.array(index)))\n",
    "        index = to_categorical(index, num_classes=10)\n",
    "        #print(np.shape(np.array(index)))\n",
    "        #print(index[i,:])\n",
    "        pred_new += index\n",
    "        #print(pred_new[0:3,:])\n",
    "\n",
    "    pred_new = np.expand_dims(pred_new, axis=1)\n",
    "    pred_new = np.argmax(pred_new, axis=2)\n",
    "    max_voting_accuracies[k] = np.sum(np.equal(pred_new, y_test)) / y_test.shape[0]\n",
    "    print(k,\"test_acc:\", max_voting_accuracies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('CIFAR10_max_voting_accuracies.txt', max_voting_accuracies, fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average\n",
    "\n",
    "average_accuracies = np.zeros((51,1))\n",
    "\n",
    "for i in np.arange(2,50):\n",
    "    n_models = i\n",
    "    models_short = models_bag[0:n_models]\n",
    "    average_model = average_ensemble(models_short, model_input)\n",
    "    average_accuracies[i] = evaluate_test_accuracy(average_model)\n",
    "    print(i,\"test_acc:\", average_accuracies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('CIFAR10_average_accuracies.txt', average_accuracies, fmt='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Non-bagging (evaluate simple combiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max voting\n",
    "\n",
    "models_short = models_nonbag[0:5]\n",
    "\n",
    "pred_new = np.zeros((10000,10))\n",
    "max_voting_model = concatenate_ensemble(models_short, model_input)\n",
    "pred = max_voting_model.predict(x_test, batch_size = 256)\n",
    "pred = np.expand_dims(pred, axis=1)\n",
    "for i in range(len(models_short)):\n",
    "    j = i*10\n",
    "    #print(i)\n",
    "    #print(np.shape(np.array(pred)))\n",
    "    #print(np.shape(np.array(pred)))\n",
    "    pred_part = pred[:,:,j:j+10]\n",
    "    #print(np.shape(np.array(pred)))\n",
    "    index = np.argmax(pred_part, axis=2)\n",
    "    #print(np.shape(np.array(index)))\n",
    "    index = to_categorical(index, num_classes=10)\n",
    "    #print(np.shape(np.array(index)))\n",
    "    #print(index[i,:])\n",
    "    pred_new += index\n",
    "    #print(pred_new[0:3,:])\n",
    "\n",
    "pred_new = np.expand_dims(pred_new, axis=1)\n",
    "pred_new = np.argmax(pred_new, axis=2)\n",
    "max_voting_accuracy = np.sum(np.equal(pred_new, y_test)) / y_test.shape[0]\n",
    "print(\"max voting, test acc:\", max_voting_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average\n",
    "\n",
    "models_short = models_nonbag[0:5]\n",
    "\n",
    "average_model = average_ensemble(models_short, model_input)\n",
    "average_accuracy = evaluate_test_accuracy(average_model)\n",
    "print(\"average, test acc:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train and evaluate stacking combiners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Bagging (train and evaluate stacking combiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble once\n",
    "\n",
    "n_models = 3\n",
    "\n",
    "ensemble_model = basic_stacking_ensemble(models_bag[0:n_models], model_input, n_models)\n",
    "_ = compile_and_train(ensemble_model, num_epochs=1)\n",
    "accuracy = evaluate_test_accuracy(ensemble_model)\n",
    "print(n_models, \"test_acc:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble multiple times\n",
    "\n",
    "basic_stacking_accuracies = np.zeros((51,1))\n",
    "\n",
    "range_list = np.arange(0,50,5) \n",
    "range_list[0] = 2 # 2,5,10,15,20,...,45\n",
    "\n",
    "for i in range_list:\n",
    "    n_models = i\n",
    "    ensemble_model = basic_stacking_ensemble(models_bag[0:n_models], model_input, i)\n",
    "    _ = compile_and_train(ensemble_model, num_epochs=3)\n",
    "    basic_stacking_accuracies[i] = evaluate_test_accuracy(ensemble_model)\n",
    "    print(i,\"test_acc:\", basic_stacking_accuracies[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('CIFAR10_basic_stacking_accuracies.txt', basic_stacking_accuracies, fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble\n",
    "\n",
    "n_models = 15\n",
    "\n",
    "ensemble_model = basic_stacking_ensemble(models_bag[0:n_models], model_input)\n",
    "ensemble_model.load_weights('weights/CIFAR10/CIFAR10_basic_stacking_ensemble.01-0.99.hdf5')\n",
    "print(evaluate_test_accuracy(ensemble_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2.4.2 Non-bagging (train and evaluate stacking combiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble once and evaluate\n",
    "\n",
    "models_short = models_nonbag[0:5]\n",
    "\n",
    "ensemble_model = pop_stacking_ensemble(models_short, model_input, 0)\n",
    "_ = compile_and_train(ensemble_model, num_epochs=20)\n",
    "accuracy = evaluate_test_accuracy(ensemble_model)\n",
    "print(len(models_short), \"test_acc:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble\n",
    "\n",
    "models_short = models_nonbag[0:5]\n",
    "\n",
    "ensemble_model = pop_stacking_ensemble(models_short, model_input, 0)\n",
    "print(ensemble_model.summary())\n",
    "ensemble_model.load_weights('weights/CIFAR10/CIFAR10_pop_stacking_0_ensemble.05-0.77.hdf5')\n",
    "print(evaluate_test_accuracy(ensemble_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
